{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "FINAL_CSV = \"../data/products_clean.csv\"\n",
    "EMBEDDINGS_FILE = \"../data/product_embeddings.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FINAL_CSV)\n",
    "print(\"Clean dataset loaded:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine text fields\n",
    "text_fields = ['title', 'description', 'categories', 'brand', 'manufacturer', 'material', 'color', 'country_of_origin']\n",
    "\n",
    "def combine_text(row):\n",
    "    categories = ' '.join(eval(row['categories'])) if pd.notnull(row['categories']) else ''\n",
    "    return f\"{row['title']} {row['description']} {categories} {row['brand']} {row['manufacturer']} {row['material']} {row['color']} {row['country_of_origin']}\"\n",
    "\n",
    "df['text_for_embedding'] = df.apply(combine_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embeddings\n",
    "text_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# save model for main.py\n",
    "text_model.save(\"../models/text_model\")\n",
    "\n",
    "text_embeddings = text_model.encode(df['text_for_embedding'].tolist(), show_progress_bar=True)\n",
    "print(\"Text embeddings shape:\", text_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "weights = ViT_B_16_Weights.DEFAULT\n",
    "image_model = vit_b_16(weights=weights)\n",
    "image_model.eval()\n",
    "\n",
    "# save model for main.py\n",
    "torch.save(image_model.state_dict(), \"../models/image_model.pth\")\n",
    "\n",
    "# Image transforms\n",
    "transform = weights.transforms()\n",
    "\n",
    "def get_image_embedding(url):\n",
    "    try:\n",
    "        response = requests.get(url.strip())\n",
    "        img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "        img_t = transform(img).unsqueeze(0)  # add batch dim\n",
    "        with torch.no_grad():\n",
    "            emb = image_model(img_t)\n",
    "        return emb.squeeze().numpy()\n",
    "    except:\n",
    "        return np.zeros(768)  # fallback vector\n",
    "\n",
    "df['image_url'] = df['images'].apply(lambda x: eval(x)[0] if len(eval(x))>0 else \"\")\n",
    "image_embeddings = np.stack(df['image_url'].apply(get_image_embedding).to_list())\n",
    "print(\"Image embeddings shape:\", image_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Save embeddings with meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings with uniq_id\n",
    "embedding_df = pd.DataFrame({\n",
    "    'uniq_id': df['uniq_id'],\n",
    "    'title': df['title'],\n",
    "    'brand': df['brand'],\n",
    "    'price': df['price'],\n",
    "    'color': df['color'],\n",
    "    'material': df['material'],\n",
    "    'country_of_origin': df['country_of_origin'],\n",
    "    'package_dimensions': df['package_dimensions'], \n",
    "    'image_url': df['image_url'],\n",
    "    'text_embedding': list(text_embeddings),\n",
    "    'image_embedding': list(image_embeddings)\n",
    "})\n",
    "\n",
    "embedding_df.to_parquet(EMBEDDINGS_FILE, index=False)\n",
    "print(\"Embeddings saved to:\", EMBEDDINGS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df.head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
